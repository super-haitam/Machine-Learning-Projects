{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0583b50d-9345-4e6c-a808-6663763c3161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment\n",
    "import gym\n",
    "\n",
    "# Neural Networks\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6714a3f-762f-4090-963d-558af3a5e886",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1dd3fef-808b-40ae-9be1-19d4158ab506",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_states = env.observation_space.shape[0]\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85abed5a-f877-4f9b-b737-8a2352c0d42b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_states, nb_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a86bc23-9d94-493d-9fed-c932e2ca2d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_hidden_nodes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477b600a-aca0-4b57-90e7-9e10d97473a4",
   "metadata": {},
   "source": [
    "# Create a Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f32992dd-bcff-446c-8fed-1d26b19c3665",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, nb_states, nb_hidden_nodes, nb_actions):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.sq = nn.Sequential(\n",
    "            nn.Linear(nb_states, nb_hidden_nodes),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(nb_hidden_nodes, nb_hidden_nodes),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(nb_hidden_nodes, nb_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # print(\"x:\", x.shape, x.dtype)\n",
    "        return self.sq(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72557ef-d87c-4814-a854-3995314328a3",
   "metadata": {},
   "source": [
    "# Fit Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27d89954-3f9e-4a0b-a6c2-179562a6ef1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model: nn.Module, X: torch.Tensor, y: torch.Tensor, epochs: int):\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    # print(\"X:\", X.shape, X.dtype)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        y_pred = model(X)\n",
    "\n",
    "        cost = loss(y, y_pred)\n",
    "        cost.backward()\n",
    "\n",
    "        optim.step()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ab9580-a4e1-4509-955f-b6cb10b2aaa1",
   "metadata": {},
   "source": [
    "# Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "786c0a7b-12f9-4c23-b6c7-f27c2af6e4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_episodes = 1_000\n",
    "nb_steps = 100\n",
    "\n",
    "gamma = 0.9\n",
    "\n",
    "epsilon = 1\n",
    "epsilon_decay = 0.002\n",
    "epsilon_min = 0.01\n",
    "\n",
    "max_mem_size = 100_000\n",
    "\n",
    "minibatch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1c8eb7-7ab2-4ee4-a35f-a28fa90d201c",
   "metadata": {},
   "source": [
    "# Replay Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26ee3c82-3309-4c28-bffd-488e80bf4a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replay(replay_memory, minibatch_size):\n",
    "    minibatch = np.random.choice(replay_memory, minibatch_size, replace=True)\n",
    "\n",
    "    # Get all the states, actions, rewards, next_states, dones as arrays\n",
    "    array_state =      np.array(list(map(lambda x: x[\"state\"], minibatch)))\n",
    "    array_action =     np.array(list(map(lambda x: x[\"action\"], minibatch)))\n",
    "    array_reward =     np.array(list(map(lambda x: x[\"reward\"], minibatch)))\n",
    "    array_next_state = np.array(list(map(lambda x: x[\"next_state\"], minibatch)))\n",
    "    array_done =       np.array(list(map(lambda x: x[\"done\"], minibatch)))\n",
    "\n",
    "    pred_next_state_actions = model( torch.tensor(array_next_state) )\n",
    "\n",
    "    state_actions = model( torch.tensor(array_state) )\n",
    "\n",
    "    for i, (state, action, reward, next_state, done) in enumerate(\n",
    "        zip(array_state, array_action, array_reward, pred_next_state_actions, array_done)\n",
    "    ):\n",
    "        if not done:\n",
    "            target = reward + gamma * torch.max(next_state)\n",
    "        else:\n",
    "            target = reward\n",
    "\n",
    "        state_actions[i][action] = target\n",
    "\n",
    "    # print(\"array_state:\", array_state.shape, array_state.dtype)\n",
    "    # print(\"state_actions:\", state_actions.shape, state_actions.dtype)\n",
    "\n",
    "    return fit_model(model, torch.tensor(array_state), state_actions, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280b2ee6-71d9-43cd-b557-b51a1b77b5fb",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c0e917e-1a35-4bca-9b5e-3443bc76b10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(nb_states, nb_hidden_nodes, nb_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fe88117-0f46-48f6-b90e-8a3ca3aec3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose_freq = 1 / 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c94fbab-af26-47c0-bb67-bc42ef72a7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0% | Episode: 0 | Epsilon: 0.998 | Avg. Reward: -100.0\n",
      "5.0% | Episode: 50 | Epsilon: 0.898 | Avg. Reward: -100.0\n",
      "10.0% | Episode: 100 | Epsilon: 0.798 | Avg. Reward: -100.0\n",
      "15.0% | Episode: 150 | Epsilon: 0.698 | Avg. Reward: -100.0\n",
      "20.0% | Episode: 200 | Epsilon: 0.598 | Avg. Reward: -100.0\n",
      "25.0% | Episode: 250 | Epsilon: 0.498 | Avg. Reward: -100.0\n",
      "30.0% | Episode: 300 | Epsilon: 0.398 | Avg. Reward: -100.0\n",
      "35.0% | Episode: 350 | Epsilon: 0.298 | Avg. Reward: -100.0\n",
      "40.0% | Episode: 400 | Epsilon: 0.198 | Avg. Reward: -100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "replay_memory = []\n",
    "all_episode_rewards = []\n",
    "\n",
    "for episode in range(nb_episodes):\n",
    "    state = env.reset()[0]\n",
    "    episode_reward = 0\n",
    "    \n",
    "    for step in range(nb_steps):\n",
    "        if np.random.uniform(0, 1) < epsilon: # EXPLORATION\n",
    "            action = random.randrange(0, nb_actions)\n",
    "        else:\n",
    "            action = model( torch.tensor(state) ).detach().numpy().argmax()    # EXPLOITATION\n",
    "\n",
    "        # Apply the actin to the environment\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        replay_memory.append({\"state\": state, \"action\": action, \"reward\": reward, \"next_state\": next_state, \"done\": done})\n",
    "        episode_reward += reward\n",
    "\n",
    "        if len(replay_memory) > max_mem_size:\n",
    "            replay_memory.pop(0)\n",
    "\n",
    "        model = replay(replay_memory, minibatch_size=32)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    # Record Episode Rewards\n",
    "    all_episode_rewards.append(episode_reward)\n",
    "\n",
    "    # Decay the epsilon\n",
    "    epsilon = max(epsilon - epsilon_decay, epsilon_min)\n",
    "\n",
    "    # print(episode)\n",
    "    if episode % int(nb_episodes * verbose_freq) == 0:\n",
    "        print(f\"{(episode/nb_episodes) * 100:.1f}% | Episode: {episode} | Epsilon: {epsilon:.3f} | Avg. Reward: {np.mean(all_episode_rewards[-int(nb_episodes * verbose_freq):])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8adc9e-589a-42d8-b4b4-f068ef99c2e3",
   "metadata": {},
   "source": [
    "# Plotting the Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fbb7e9-709c-4a2e-a743-cef7a3d363f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter([i for i in range(len(all_episode_rewards))], all_episode_rewards)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9d3dd2-3ee5-47a7-a9ac-d91bb690883c",
   "metadata": {},
   "source": [
    "# Visualizing The Agent while Playing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8edeae-7c20-46d0-a303-41b30a292000",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\", render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487cc5eb-fab9-4f8c-8047-9f1ec03d7275",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_test_episodes = 1\n",
    "nb_test_step = nb_steps * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75bfe39-d023-446a-adea-94c25a3bb3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(nb_test_episodes):\n",
    "    state = env.reset()[0]\n",
    "    \n",
    "    for step in range(nb_test_step):\n",
    "        env.render()\n",
    "\n",
    "        action = model( torch.tensor(state) ).detach().numpy().argmax()\n",
    "\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        state = next_state\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbf1596-48a1-4c49-be0f-25260bfab047",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
