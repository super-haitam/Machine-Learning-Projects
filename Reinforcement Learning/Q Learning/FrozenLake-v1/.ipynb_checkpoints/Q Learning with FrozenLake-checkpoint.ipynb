{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q* Learning with FrozenLake üïπÔ∏è‚õÑ\n",
    "<br> \n",
    "In this Notebook, we'll implement an agent <b>that plays FrozenLake.</b>\n",
    "<img src=\"frozenlake.png\" alt=\"Frozen Lake\"/>\n",
    "\n",
    "The goal of this game is <b>to go from the starting state (S) to the goal state (G)</b> by walking only on frozen tiles (F) and avoid holes (H).However, the ice is slippery, <b>so you won't always move in the direction you intend (stochastic environment)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites üèóÔ∏è\n",
    "Before diving on the notebook **you need to understand**:\n",
    "- The foundations of Reinforcement learning (MC, TD, Rewards hypothesis...) [Article](https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419)\n",
    "- Q-learning [Article](https://medium.freecodecamp.org/diving-deeper-into-reinforcement-learning-with-q-learning-c18d0db58efe)\n",
    "- In the [video version](https://www.youtube.com/watch?v=q2ZOEFAaaI0)  we implemented a Q-learning agent that learns to play OpenAI Taxi-v2 üöï with Numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Import the dependencies üìö\n",
    "We use 3 libraries:\n",
    "- `Numpy` for our Qtable\n",
    "- `OpenAI Gym` for our FrozenLake Environment\n",
    "- `Random` to generate random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create the environment üéÆ\n",
    "- Here we'll create the FrozenLake environment. \n",
    "- OpenAI Gym is a library <b> composed of many environments that we can use to train our agents.</b>\n",
    "- In our case we choose to use Frozen Lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", render_mode=\"rgb_array\", is_slippery=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create the Q-table and initialize it üóÑÔ∏è\n",
    "- Now, we'll create our Q-table, to know how much rows (states) and columns (actions) we need, we need to calculate the action_size and the state_size\n",
    "- OpenAI Gym provides us a way to do that: `env.action_space.n` and `env.observation_space.n`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_size, state_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "qtable = np.zeros((state_size, action_size))\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create the hyperparameters ‚öôÔ∏è\n",
    "- Here, we'll specify the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_episodes = 10_000        # Total episodes\n",
    "learning_rate = 0.8           # Learning rate\n",
    "max_steps = 99                # Max steps per episode\n",
    "gamma = 0.95                  # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 1.0                 # Exploration rate\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0           # Minimum exploration probability \n",
    "decay_rate = 0.0005            # Linear decay rate for exploration prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: The Q learning algorithm üß†\n",
    "- Now we implement the Q learning algorithm:\n",
    "<img src=\"qtable_algo.png\" alt=\"Q algo\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 | Episode Reward: 0.0 | Epsilon: 0.99950 | Mean Reward For Last 1000 Episodes: 0.0\n",
      "Episode: 1000 | Episode Reward: 1.0 | Epsilon: 0.49950 | Mean Reward For Last 1000 Episodes: 0.154\n",
      "Episode: 2000 | Episode Reward: 1.0 | Epsilon: 0.00000 | Mean Reward For Last 1000 Episodes: 0.714\n",
      "Episode: 3000 | Episode Reward: 1.0 | Epsilon: 0.00000 | Mean Reward For Last 1000 Episodes: 1.0\n",
      "Episode: 4000 | Episode Reward: 1.0 | Epsilon: 0.00000 | Mean Reward For Last 1000 Episodes: 1.0\n",
      "Episode: 5000 | Episode Reward: 1.0 | Epsilon: 0.00000 | Mean Reward For Last 1000 Episodes: 1.0\n",
      "Episode: 6000 | Episode Reward: 1.0 | Epsilon: 0.00000 | Mean Reward For Last 1000 Episodes: 1.0\n",
      "Episode: 7000 | Episode Reward: 1.0 | Epsilon: 0.00000 | Mean Reward For Last 1000 Episodes: 1.0\n",
      "Episode: 8000 | Episode Reward: 1.0 | Epsilon: 0.00000 | Mean Reward For Last 1000 Episodes: 1.0\n",
      "Episode: 9000 | Episode Reward: 1.0 | Epsilon: 0.00000 | Mean Reward For Last 1000 Episodes: 1.0\n",
      "Episode: 9999 | Episode Reward: 1.0 | Epsilon: 0.00000 | Mean Reward For Last 1000 Episodes: 1.0\n",
      "Considering All Episodes: 0.89% Succeeded | 0.11% Failed\n",
      "Considering last 3000 Episodes: 0.62% Succeeded | 0.38% Failed\n"
     ]
    }
   ],
   "source": [
    "# List of rewards\n",
    "rewards = []\n",
    "tenthEpisodeRewards = []\n",
    "\n",
    "# 2 For life or until learning is stopped\n",
    "for episode in range(total_episodes):\n",
    "    # Reset the environment\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]\n",
    "             \n",
    "        # 3. Choose an action a in the current world state (s)\n",
    "        ## First we randomize a number\n",
    "        exp_exp_tradeoff = random.uniform(0, 1)\n",
    "        \n",
    "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
    "        if exp_exp_tradeoff > epsilon:\n",
    "            action = np.argmax(qtable[state,:])\n",
    "\n",
    "        # Else doing a random choice --> exploration\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "        new_state, reward, done, info, _ = env.step(action)\n",
    "\n",
    "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "        # qtable[new_state,:] : all the actions we can take from new state\n",
    "        \n",
    "        qtable[state, action] = (1 - learning_rate) * qtable[state, action] + learning_rate * ( reward + gamma * np.max(qtable[new_state, :]) )\n",
    "        \n",
    "        total_rewards += reward\n",
    "        \n",
    "        # Our new state is state\n",
    "        state = new_state\n",
    "        \n",
    "        # If done (if we're dead) : finish episode\n",
    "        if done: \n",
    "            break\n",
    "    \n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    # epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
    "    epsilon = max( epsilon - decay_rate, min_epsilon )\n",
    "    rewards.append(total_rewards)\n",
    "    tenthEpisodeRewards.append(total_rewards)\n",
    "\n",
    "    if episode % (total_episodes//10) == 0 or episode == total_episodes-1:\n",
    "        print(f\"Episode: {episode} | Episode Reward: {total_rewards} | Epsilon: {epsilon:.5f} | Mean Reward For Last {total_episodes//10} Episodes: {np.mean(tenthEpisodeRewards)}\")\n",
    "        if episode != total_episodes-1:\n",
    "            tenthEpisodeRewards = []\n",
    "\n",
    "print(f\"Considering All Episodes: {np.sum(rewards)/len(rewards):.2f}% Succeeded | {(len(rewards)-np.sum(rewards))/len(rewards):.2f}% Failed\")\n",
    "slice = (total_episodes//10)*3\n",
    "r = rewards[:slice]\n",
    "print(f\"Considering last {slice} Episodes: {np.sum(r)/len(r):.2f}% Succeeded | {1-np.sum(r)/len(r):.2f}% Failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAiGUlEQVR4nO3de3BU5eH/8U+um2RgN2DMhstiUFFUEDCBGNB+x69bU2Sw9JoiBUy9FIoWSKsSucRWJdQLpZVoKuNtfhVBHLFWafjRCFo0khKIgty0iMmgG6BINgRMIPv8/vDn6pZAs3GTJwnv18zO6NnnnH3Og5L3nN09iTLGGAEAAFgSbXsCAADg7EaMAAAAq4gRAABgFTECAACsIkYAAIBVxAgAALCKGAEAAFYRIwAAwKpY2xNojUAgoE8++UQ9e/ZUVFSU7ekAAIBWMMaovr5effv2VXT06a9/dIkY+eSTT+TxeGxPAwAAtEFNTY369+9/2ue7RIz07NlT0hcn43Q6Lc8GAAC0ht/vl8fjCf4cP50uESNfvjXjdDqJEQAAupj/9hELPsAKAACsIkYAAIBVxAgAALCKGAEAAFYRIwAAwCpiBAAAWEWMAAAAq4gRAABgVZe46Vl7ajoZ0NNv7dX/fb9Wdccb5YiNVUJcjDy9k/Tdy/tqz8GjKv+gVpv2faamZik2WrootYeiZPSvQw1qPCGZKCkuWnLERssRF6uYKKOYKOmzz5slSX16xulok9Gh+iad1BeLftLqWQMAcKqXp43R8PTkDn/dKGOMCWeHN998Uw899JAqKyv16aefavXq1ZowYcIZ99mwYYPy8/P1/vvvy+PxaN68ebrpppta/Zp+v18ul0t1dXURvQNr0Zod+tObH0XseAAAdAf7Fo2LyHFa+/M77LdpGhoaNGzYMBUXF7dq/EcffaRx48bpmmuuUVVVlWbNmqVbbrlFa9euDfelI4oQAQCgZelzXuvQ1wv7bZqxY8dq7NixrR5fUlKigQMH6pFHHpEkXXLJJdq4caN+//vfKycnJ9yXj4imkwFCBACAM6jad6TD3rJp9w+wlpeXy+v1hmzLyclReXn5afdpbGyU3+8PeUTS/ynfF9HjAQDQ3UwoeavDXqvdY8Tn88ntdodsc7vd8vv9On78eIv7FBUVyeVyBR8ejyeic/r48LGIHg8AALRdp/xqb0FBgerq6oKPmpqaiB7/vN5JET0eAABou3aPkbS0NNXW1oZsq62tldPpVGJiYov7OBwOOZ3OkEckTc5Oj+jxAADobl6eNqbDXqvdYyQ7O1tlZWUh29atW6fs7Oz2funTio+N1s+/NdDa6wMA0Nl15P1Gwo6Ro0ePqqqqSlVVVZK++OpuVVWVqqurJX3xFsuUKVOC46dNm6a9e/fqrrvu0q5du/TYY4/phRde0OzZsyNzBm1UcP2lBAkAAC2I1H1GWivsm55t2LBB11xzzSnbp06dqmeeeUY33XST9u3bpw0bNoTsM3v2bO3YsUP9+/fX/PnzO8VNzyTuwAoAwJcifQfW1v78DjtGbGjPGAEAAO2j3e7ACgAAEEnECAAAsIoYAQAAVhEjAADAKmIEAABYRYwAAACriBEAAGAVMQIAAKwiRgAAgFXECAAAsIoYAQAAVhEjAADAKmIEAABYRYwAAACriBEAAGAVMQIAAKwiRgAAgFXECAAAsIoYAQAAVhEjAADAKmIEAABYRYwAAACriBEAAGAVMQIAAKwiRgAAgFXECAAAsIoYAQAAVhEjAADAKmIEAABYRYwAAACriBEAAGAVMQIAAKwiRgAAgFXECAAAsIoYAQAAVhEjAADAKmIEAABYRYwAAACriBEAAGAVMQIAAKwiRgAAgFXECAAAsIoYAQAAVhEjAADAKmIEAABYRYwAAACriBEAAGAVMQIAAKwiRgAAgFXECAAAsIoYAQAAVhEjAADAKmIEAABYRYwAAACriBEAAGAVMQIAAKwiRgAAgFVtipHi4mKlp6crISFBWVlZqqioOOP4JUuW6OKLL1ZiYqI8Ho9mz56tzz//vE0TBgAA3UvYMbJy5Url5+ersLBQW7Zs0bBhw5STk6MDBw60OH758uWaM2eOCgsLtXPnTj355JNauXKl7rnnnm88eQAA0PWFHSOLFy/Wrbfeqry8PF166aUqKSlRUlKSnnrqqRbHv/322xozZoxuvPFGpaen67rrrtPEiRP/69UUAABwdggrRpqamlRZWSmv1/vVAaKj5fV6VV5e3uI+o0ePVmVlZTA+9u7dqzVr1uj6668/7es0NjbK7/eHPAAAQPcUG87gQ4cOqbm5WW63O2S72+3Wrl27Wtznxhtv1KFDh3TVVVfJGKOTJ09q2rRpZ3ybpqioSL/5zW/CmRoAAOii2v3bNBs2bNDChQv12GOPacuWLXrppZf02muv6b777jvtPgUFBaqrqws+ampq2nuaAADAkrCujKSkpCgmJka1tbUh22tra5WWltbiPvPnz9fkyZN1yy23SJKGDh2qhoYG3XbbbZo7d66io0/tIYfDIYfDEc7UAABAFxXWlZH4+HhlZGSorKwsuC0QCKisrEzZ2dkt7nPs2LFTgiMmJkaSZIwJd74AAKCbCevKiCTl5+dr6tSpyszM1KhRo7RkyRI1NDQoLy9PkjRlyhT169dPRUVFkqTx48dr8eLFGjFihLKysvThhx9q/vz5Gj9+fDBKAADA2SvsGMnNzdXBgwe1YMEC+Xw+DR8+XKWlpcEPtVZXV4dcCZk3b56ioqI0b9487d+/X+eee67Gjx+vBx54IHJnAQAAuqwo0wXeK/H7/XK5XKqrq5PT6bQ9HQAA0Aqt/fnN76YBAABWESMAAMAqYgQAAFhFjAAAAKuIEQAAYBUxAgAArCJGAACAVcQIAACwihgBAABWESMAAMAqYgQAAFhFjAAAAKuIEQAAYBUxAgAArCJGAACAVcQIAACwihgBAABWESMAAMAqYgQAAFhFjAAAAKuIEQAAYBUxAgAArCJGAACAVcQIAACwihgBAABWESMAAMAqYgQAAFhFjAAAAKuIEQAAYBUxAgAArCJGAACAVcQIAACwihgBAABWESMAAMAqYgQAAFhFjAAAAKuIEQAAYBUxAgAArCJGAACAVcQIAACwihgBAABWESMAAMAqYgQAAFhFjAAAAKuIEQAAYBUxAgAArCJGAACAVcQIAACwihgBAABWESMAAMAqYgQAAFhFjAAAAKuIEQAAYBUxAgAArCJGAACAVcQIAACwqk0xUlxcrPT0dCUkJCgrK0sVFRVnHH/kyBHNmDFDffr0kcPh0EUXXaQ1a9a0acIAAKB7iQ13h5UrVyo/P18lJSXKysrSkiVLlJOTo927dys1NfWU8U1NTfr2t7+t1NRUvfjii+rXr58+/vhjJScnR2L+AACgi4syxphwdsjKytLIkSO1dOlSSVIgEJDH49Edd9yhOXPmnDK+pKREDz30kHbt2qW4uLg2TdLv98vlcqmurk5Op7NNxwAAAB2rtT+/w3qbpqmpSZWVlfJ6vV8dIDpaXq9X5eXlLe7zyiuvKDs7WzNmzJDb7daQIUO0cOFCNTc3n/Z1Ghsb5ff7Qx4AAKB7CitGDh06pObmZrnd7pDtbrdbPp+vxX327t2rF198Uc3NzVqzZo3mz5+vRx55RPfff/9pX6eoqEgulyv48Hg84UwTAAB0Ie3+bZpAIKDU1FQ98cQTysjIUG5urubOnauSkpLT7lNQUKC6urrgo6ampr2nCQAALAnrA6wpKSmKiYlRbW1tyPba2lqlpaW1uE+fPn0UFxenmJiY4LZLLrlEPp9PTU1Nio+PP2Ufh8Mhh8MRztQAAEAXFdaVkfj4eGVkZKisrCy4LRAIqKysTNnZ2S3uM2bMGH344YcKBALBbXv27FGfPn1aDBEAAHB2Cfttmvz8fC1btkzPPvusdu7cqenTp6uhoUF5eXmSpClTpqigoCA4fvr06Tp8+LBmzpypPXv26LXXXtPChQs1Y8aMyJ0FAADossK+z0hubq4OHjyoBQsWyOfzafjw4SotLQ1+qLW6ulrR0V81jsfj0dq1azV79mxdfvnl6tevn2bOnKm77747cmcBAAC6rLDvM2ID9xkBAKDraZf7jAAAAEQaMQIAAKwiRgAAgFXECAAAsIoYAQAAVhEjAADAKmIEAABYRYwAAACriBEAAGAVMQIAAKwiRgAAgFXECAAAsIoYAQAAVhEjAADAKmIEAABYRYwAAACriBEAAGAVMQIAAKwiRgAAgFXECAAAsIoYAQAAVhEjAADAKmIEAABYRYwAAACriBEAAGAVMQIAAKwiRgAAgFXECAAAsIoYAQAAVhEjAADAKmIEAABYRYwAAACriBEAAGAVMQIAAKwiRgAAgFXECAAAsIoYAQAAVhEjAADAKmIEAABYRYwAAACriBEAAGAVMQIAAKwiRgAAgFXECAAAsIoYAQAAVhEjAADAKmIEAABYRYwAAACriBEAAGAVMQIAAKwiRgAAgFXECAAAsIoYAQAAVhEjAADAKmIEAABYRYwAAACr2hQjxcXFSk9PV0JCgrKyslRRUdGq/VasWKGoqChNmDChLS8LAAC6obBjZOXKlcrPz1dhYaG2bNmiYcOGKScnRwcOHDjjfvv27dOvf/1rXX311W2eLAAA6H7CjpHFixfr1ltvVV5eni699FKVlJQoKSlJTz311Gn3aW5u1qRJk/Sb3/xG559//jeaMAAA6F7CipGmpiZVVlbK6/V+dYDoaHm9XpWXl592v9/+9rdKTU3VzTff3KrXaWxslN/vD3kAAIDuKawYOXTokJqbm+V2u0O2u91u+Xy+FvfZuHGjnnzySS1btqzVr1NUVCSXyxV8eDyecKYJAAC6kHb9Nk19fb0mT56sZcuWKSUlpdX7FRQUqK6uLvioqalpx1kCAACbYsMZnJKSopiYGNXW1oZsr62tVVpa2inj//Wvf2nfvn0aP358cFsgEPjihWNjtXv3bl1wwQWn7OdwOORwOMKZGgAA6KLCujISHx+vjIwMlZWVBbcFAgGVlZUpOzv7lPGDBw/Wtm3bVFVVFXzccMMNuuaaa1RVVcXbLwAAILwrI5KUn5+vqVOnKjMzU6NGjdKSJUvU0NCgvLw8SdKUKVPUr18/FRUVKSEhQUOGDAnZPzk5WZJO2Q4AAM5OYcdIbm6uDh48qAULFsjn82n48OEqLS0Nfqi1urpa0dHc2BUAALROlDHG2J7Ef+P3++VyuVRXVyen02l7OgAAoBVa+/ObSxgAAMAqYgQAAFhFjAAAAKuIEQAAYBUxAgAArCJGAACAVcQIAACwihgBAABWESMAAMAqYgQAAFhFjAAAAKuIEQAAYBUxAgAArCJGAACAVcQIAACwihgBAABWESMAAMAqYgQAAFhFjAAAAKuIEQAAYBUxAgAArCJGAACAVcQIAACwihgBAABWESMAAMAqYgQAAFhFjAAAAKuIEQAAYBUxAgAArCJGAACAVcQIAACwihgBAABWESMAAMAqYgQAAFhFjAAAAKuIEQAAYBUxAgAArCJGAACAVcQIAACwihgBAABWESMAAMAqYgQAAFhFjAAAAKuIEQAAYBUxAgAArCJGAACAVcQIAACwihgBAABWESMAAMAqYgQAAFhFjAAAAKuIEQAAYBUxAgAArCJGAACAVcQIAACwihgBAABWtSlGiouLlZ6eroSEBGVlZamiouK0Y5ctW6arr75avXr1Uq9eveT1es84HgAAnF3CjpGVK1cqPz9fhYWF2rJli4YNG6acnBwdOHCgxfEbNmzQxIkTtX79epWXl8vj8ei6667T/v37v/HkAQBA1xdljDHh7JCVlaWRI0dq6dKlkqRAICCPx6M77rhDc+bM+a/7Nzc3q1evXlq6dKmmTJnSqtf0+/1yuVyqq6uT0+kMZ7oAAMCS1v78DuvKSFNTkyorK+X1er86QHS0vF6vysvLW3WMY8eO6cSJE+rdu/dpxzQ2Nsrv94c8AABA9xRWjBw6dEjNzc1yu90h291ut3w+X6uOcffdd6tv374hQfOfioqK5HK5gg+PxxPONAEAQBfSod+mWbRokVasWKHVq1crISHhtOMKCgpUV1cXfNTU1HTgLAEAQEeKDWdwSkqKYmJiVFtbG7K9trZWaWlpZ9z34Ycf1qJFi/T3v/9dl19++RnHOhwOORyOcKYGAAC6qLCujMTHxysjI0NlZWXBbYFAQGVlZcrOzj7tfg8++KDuu+8+lZaWKjMzs+2zBQAA3U5YV0YkKT8/X1OnTlVmZqZGjRqlJUuWqKGhQXl5eZKkKVOmqF+/fioqKpIk/e53v9OCBQu0fPlypaenBz9b0qNHD/Xo0SOCpwIAALqisGMkNzdXBw8e1IIFC+Tz+TR8+HCVlpYGP9RaXV2t6OivLrg8/vjjampq0g9/+MOQ4xQWFuree+/9ZrMHAABdXtj3GbGB+4wAAND1tMt9RgAAACKNGAEAAFYRIwAAwCpiBAAAWEWMAAAAq4gRAABgFTECAACsIkYAAIBVxAgAALCKGAEAAFYRIwAAwCpiBAAAWEWMAAAAq4gRAABgFTECAACsIkYAAIBVxAgAALCKGAEAAFYRIwAAwCpiBAAAWEWMAAAAq4gRAABgFTECAACsIkYAAIBVxAgAALCKGAEAAFYRIwAAwCpiBAAAWEWMAAAAq4gRAABgFTECAACsIkYAAIBVxAgAALCKGAEAAFYRIwAAwCpiBAAAWEWMAAAAq4gRAABgFTECAACsIkYAAIBVxAgAALCKGAEAAFYRIwAAwCpiBAAAWEWMAAAAq4gRAABgFTECAACsIkYAAIBVxAgAALCKGAEAAFYRIwAAwCpiBAAAWEWMAAAAq4gRAABgFTECAACsirU9AVsOH23S/96/TkdsT6SLiYuSYqOlpmYpICk2SkqKj1ZsTLSSE+MUHxst/+cn1Wyk81OS9K2LUuV2JSrNmaBRA3srJjrK9ikAADqZNl0ZKS4uVnp6uhISEpSVlaWKioozjl+1apUGDx6shIQEDR06VGvWrGnTZCNl5P3rdAUh0iYnjHS8WWqWZP7/v9c1BvTvYyf1r38f187aBu2va5TP36i3936mRaW7NXtllSYue0dX/e51lW7/1PYpAAA6mbBjZOXKlcrPz1dhYaG2bNmiYcOGKScnRwcOHGhx/Ntvv62JEyfq5ptv1tatWzVhwgRNmDBB27dv/8aTb4uR96/TwaNNVl77bPdp3eea/uctBAkAIESUMcaEs0NWVpZGjhyppUuXSpICgYA8Ho/uuOMOzZkz55Txubm5amho0KuvvhrcduWVV2r48OEqKSlp1Wv6/X65XC7V1dXJ6XSGM90Qh4826Yr717V5f3xzUZLSXAnaePf/8pYNAHRzrf35HdaVkaamJlVWVsrr9X51gOhoeb1elZeXt7hPeXl5yHhJysnJOe14SWpsbJTf7w95RMJPnng7IsdB2xl9cYWk4qPDtqcCAOgkwoqRQ4cOqbm5WW63O2S72+2Wz+drcR+fzxfWeEkqKiqSy+UKPjweTzjTPK0D9bw901kcqP/c9hQAAJ1Ep/xqb0FBgerq6oKPmpqaiBw3tWd8RI6Dby61Z4LtKQAAOomwYiQlJUUxMTGqra0N2V5bW6u0tLQW90lLSwtrvCQ5HA45nc6QRySsuG10RI6DtouS1Mf1xdd8AQCQwoyR+Ph4ZWRkqKysLLgtEAiorKxM2dnZLe6TnZ0dMl6S1q1bd9rx7al3j3id24OrI7YVjr+UD68CAILCfpsmPz9fy5Yt07PPPqudO3dq+vTpamhoUF5eniRpypQpKigoCI6fOXOmSktL9cgjj2jXrl269957tXnzZt1+++2RO4sw/HPetwkSS/q4EvT4T6/Qd4b0sT0VAEAnEvYdWHNzc3Xw4EEtWLBAPp9Pw4cPV2lpafBDqtXV1YqO/qpxRo8ereXLl2vevHm65557NGjQIL388ssaMmRI5M4iTP+c923uwNpG3IEVABBpYd9nxIZI3WcEAAB0nHa5zwgAAECkESMAAMAqYgQAAFhFjAAAAKuIEQAAYBUxAgAArCJGAACAVcQIAACwihgBAABWhX07eBu+vEms3++3PBMAANBaX/7c/m83e+8SMVJfXy9J8ng8lmcCAADCVV9fL5fLddrnu8TvpgkEAvrkk0/Us2dPRUVF7het+f1+eTwe1dTU8Dtv2hHr3HFY647BOncM1rljtOc6G2NUX1+vvn37hvwS3f/UJa6MREdHq3///u12fKfTyX/oHYB17jisdcdgnTsG69wx2mudz3RF5Et8gBUAAFhFjAAAAKvO6hhxOBwqLCyUw+GwPZVujXXuOKx1x2CdOwbr3DE6wzp3iQ+wAgCA7uusvjICAADsI0YAAIBVxAgAALCKGAEAAFad1TFSXFys9PR0JSQkKCsrSxUVFban1GkVFRVp5MiR6tmzp1JTUzVhwgTt3r07ZMznn3+uGTNm6JxzzlGPHj30gx/8QLW1tSFjqqurNW7cOCUlJSk1NVV33nmnTp48GTJmw4YNuuKKK+RwOHThhRfqmWeeae/T67QWLVqkqKgozZo1K7iNdY6M/fv366c//anOOeccJSYmaujQodq8eXPweWOMFixYoD59+igxMVFer1cffPBByDEOHz6sSZMmyel0Kjk5WTfffLOOHj0aMua9997T1VdfrYSEBHk8Hj344IMdcn6dQXNzs+bPn6+BAwcqMTFRF1xwge67776Q31PCOrfNm2++qfHjx6tv376KiorSyy+/HPJ8R67rqlWrNHjwYCUkJGjo0KFas2ZN+CdkzlIrVqww8fHx5qmnnjLvv/++ufXWW01ycrKpra21PbVOKScnxzz99NNm+/btpqqqylx//fVmwIAB5ujRo8Ex06ZNMx6Px5SVlZnNmzebK6+80owePTr4/MmTJ82QIUOM1+s1W7duNWvWrDEpKSmmoKAgOGbv3r0mKSnJ5Ofnmx07dphHH33UxMTEmNLS0g49386goqLCpKenm8svv9zMnDkzuJ11/uYOHz5szjvvPHPTTTeZTZs2mb1795q1a9eaDz/8MDhm0aJFxuVymZdfftm8++675oYbbjADBw40x48fD475zne+Y4YNG2beeecd849//MNceOGFZuLEicHn6+rqjNvtNpMmTTLbt283zz//vElMTDR/+tOfOvR8bXnggQfMOeecY1599VXz0UcfmVWrVpkePXqYP/zhD8ExrHPbrFmzxsydO9e89NJLRpJZvXp1yPMdta5vvfWWiYmJMQ8++KDZsWOHmTdvnomLizPbtm0L63zO2hgZNWqUmTFjRvDfm5ubTd++fU1RUZHFWXUdBw4cMJLMG2+8YYwx5siRIyYuLs6sWrUqOGbnzp1GkikvLzfGfPE/T3R0tPH5fMExjz/+uHE6naaxsdEYY8xdd91lLrvsspDXys3NNTk5Oe19Sp1KfX29GTRokFm3bp35n//5n2CMsM6Rcffdd5urrrrqtM8HAgGTlpZmHnrooeC2I0eOGIfDYZ5//nljjDE7duwwksw///nP4Ji//e1vJioqyuzfv98YY8xjjz1mevXqFVz3L1/74osvjvQpdUrjxo0zP/vZz0K2ff/73zeTJk0yxrDOkfKfMdKR6/rjH//YjBs3LmQ+WVlZ5uc//3lY53BWvk3T1NSkyspKeb3e4Lbo6Gh5vV6Vl5dbnFnXUVdXJ0nq3bu3JKmyslInTpwIWdPBgwdrwIABwTUtLy/X0KFD5Xa7g2NycnLk9/v1/vvvB8d8/Rhfjjnb/lxmzJihcePGnbIWrHNkvPLKK8rMzNSPfvQjpaamasSIEVq2bFnw+Y8++kg+ny9kjVwul7KyskLWOTk5WZmZmcExXq9X0dHR2rRpU3DMt771LcXHxwfH5OTkaPfu3frss8/a+zStGz16tMrKyrRnzx5J0rvvvquNGzdq7Nixkljn9tKR6xqpv0vOyhg5dOiQmpubQ/6yliS32y2fz2dpVl1HIBDQrFmzNGbMGA0ZMkSS5PP5FB8fr+Tk5JCxX19Tn8/X4pp/+dyZxvj9fh0/frw9TqfTWbFihbZs2aKioqJTnmOdI2Pv3r16/PHHNWjQIK1du1bTp0/XL3/5Sz377LOSvlqnM/0d4fP5lJqaGvJ8bGysevfuHdafRXc2Z84c/eQnP9HgwYMVFxenESNGaNasWZo0aZIk1rm9dOS6nm5MuOveJX5rLzqXGTNmaPv27dq4caPtqXQ7NTU1mjlzptatW6eEhATb0+m2AoGAMjMztXDhQknSiBEjtH37dpWUlGjq1KmWZ9d9vPDCC3ruuee0fPlyXXbZZaqqqtKsWbPUt29f1hkhzsorIykpKYqJiTnlGwi1tbVKS0uzNKuu4fbbb9err76q9evXq3///sHtaWlpampq0pEjR0LGf31N09LSWlzzL5870xin06nExMRIn06nU1lZqQMHDuiKK65QbGysYmNj9cYbb+iPf/yjYmNj5Xa7WecI6NOnjy699NKQbZdccomqq6slfbVOZ/o7Ii0tTQcOHAh5/uTJkzp8+HBYfxbd2Z133hm8OjJ06FBNnjxZs2fPDl71Y53bR0eu6+nGhLvuZ2WMxMfHKyMjQ2VlZcFtgUBAZWVlys7OtjizzssYo9tvv12rV6/W66+/roEDB4Y8n5GRobi4uJA13b17t6qrq4Nrmp2drW3btoX8D7Bu3To5nc7gD4bs7OyQY3w55mz5c7n22mu1bds2VVVVBR+ZmZmaNGlS8J9Z529uzJgxp3w1fc+ePTrvvPMkSQMHDlRaWlrIGvn9fm3atClknY8cOaLKysrgmNdff12BQEBZWVnBMW+++aZOnDgRHLNu3TpdfPHF6tWrV7udX2dx7NgxRUeH/piJiYlRIBCQxDq3l45c14j9XRLWx127kRUrVhiHw2GeeeYZs2PHDnPbbbeZ5OTkkG8g4CvTp083LpfLbNiwwXz66afBx7Fjx4Jjpk2bZgYMGGBef/11s3nzZpOdnW2ys7ODz3/5ldPrrrvOVFVVmdLSUnPuuee2+JXTO++80+zcudMUFxefVV85bcnXv01jDOscCRUVFSY2NtY88MAD5oMPPjDPPfecSUpKMn/+85+DYxYtWmSSk5PNX/7yF/Pee++Z7373uy1+NXLEiBFm06ZNZuPGjWbQoEEhX408cuSIcbvdZvLkyWb79u1mxYoVJikpqVt/5fTrpk6davr16xf8au9LL71kUlJSzF133RUcwzq3TX19vdm6davZunWrkWQWL15stm7daj7++GNjTMet61tvvWViY2PNww8/bHbu3GkKCwv5am+4Hn30UTNgwAATHx9vRo0aZd555x3bU+q0JLX4ePrpp4Njjh8/bn7xi1+YXr16maSkJPO9733PfPrppyHH2bdvnxk7dqxJTEw0KSkp5le/+pU5ceJEyJj169eb4cOHm/j4eHP++eeHvMbZ6D9jhHWOjL/+9a9myJAhxuFwmMGDB5snnngi5PlAIGDmz59v3G63cTgc5tprrzW7d+8OGfPvf//bTJw40fTo0cM4nU6Tl5dn6uvrQ8a8++675qqrrjIOh8P069fPLFq0qN3PrbPw+/1m5syZZsCAASYhIcGcf/75Zu7cuSFfFWWd22b9+vUt/p08depUY0zHrusLL7xgLrroIhMfH28uu+wy89prr4V9PlHGfO1WeAAAAB3srPzMCAAA6DyIEQAAYBUxAgAArCJGAACAVcQIAACwihgBAABWESMAAMAqYgQAAFhFjAAAAKuIEQAAYBUxAgAArCJGAACAVf8PIadwBND1YLQAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(20, 10))\n",
    "plt.scatter( [i for i in range(len(rewards))], rewards )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Use our Q-table to play FrozenLake ! üëæ\n",
    "- After 10 000 episodes, our Q-table can be used as a \"cheatsheet\" to play FrozenLake\"\n",
    "- By running this cell you can see our agent playing FrozenLake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", render_mode=\"human\", is_slippery=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************\n",
      "EPISODE  0\n",
      "Number of steps: 5 | Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  1\n",
      "Number of steps: 5 | Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  2\n",
      "Number of steps: 5 | Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  3\n",
      "Number of steps: 5 | Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  4\n",
      "Number of steps: 5 | Reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "for episode in range(5):\n",
    "    state = env.reset()\n",
    "    state = 10\n",
    "    step = 0\n",
    "    done = False\n",
    "    print(\"****************************************************\")\n",
    "    print(\"EPISODE \", episode)\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]\n",
    "        \n",
    "        # Take the action (index) that have the maximum expected future reward given that state\n",
    "        action = np.argmax(qtable[state,:])\n",
    "        \n",
    "        new_state, reward, done, info, _ = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            # Here, we decide to only print the last state (to see if our agent is on the goal or fall into an hole)\n",
    "            env.render()\n",
    "            \n",
    "            # We print the number of step it took.\n",
    "            print(f\"Number of steps: {step} | Reward: {reward}\")\n",
    "            break\n",
    "        state = new_state\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
